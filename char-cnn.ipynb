{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded from ./_train.csv\n",
      "Data loaded from ./_validate.csv\n",
      "Data loaded from ./_test.csv\n"
     ]
    }
   ],
   "source": [
    "from data_utils import Data\n",
    "\n",
    "\n",
    "config = json.load(open(\"./config.json\"))\n",
    "\n",
    "# Load training data\n",
    "training_data = Data(data_source=config[\"data\"][\"training_data_source\"], alphabet=config[\"data\"][\"alphabet\"],\n",
    "                         input_size=config[\"data\"][\"input_size\"],num_of_classes=config[\"data\"][\"num_of_classes\"])\n",
    "training_data.load_data()\n",
    "training_inputs, training_labels, batch_texts = training_data.get_all_data()\n",
    "\n",
    "\n",
    "# Load validation data\n",
    "validation_data = Data(data_source=config[\"data\"][\"validation_data_source\"], alphabet=config[\"data\"][\"alphabet\"],\n",
    "                           input_size=config[\"data\"][\"input_size\"], num_of_classes=config[\"data\"][\"num_of_classes\"])\n",
    "validation_data.load_data()\n",
    "validation_inputs, validation_labels, batch_texts = validation_data.get_all_data()\n",
    "\n",
    "# Load test data\n",
    "test_data = Data(data_source=config[\"data\"][\"test_data_source\"], alphabet=config[\"data\"][\"alphabet\"],\n",
    "                           input_size=config[\"data\"][\"input_size\"], num_of_classes=config[\"data\"][\"num_of_classes\"])\n",
    "test_data.load_data()\n",
    "test_inputs, test_labels, batch_texts = test_data.get_all_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropout param: 0.5\n",
      "tracking <tf.Variable 'Variable:0' shape=() dtype=int32, numpy=0> tp\n",
      "tracking <tf.Variable 'Variable:0' shape=() dtype=int32, numpy=0> fp\n",
      "tracking <tf.Variable 'Variable:0' shape=() dtype=int32, numpy=0> tp\n",
      "tracking <tf.Variable 'Variable:0' shape=() dtype=int32, numpy=0> fn\n",
      "CharCNNZhang model built: \n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sent_input (InputLayer)      (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 400, 128)          8960      \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 394, 256)          229632    \n",
      "_________________________________________________________________\n",
      "thresholded_re_lu_1 (Thresho (None, 394, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 131, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 125, 256)          459008    \n",
      "_________________________________________________________________\n",
      "thresholded_re_lu_2 (Thresho (None, 125, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 41, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 39, 256)           196864    \n",
      "_________________________________________________________________\n",
      "thresholded_re_lu_3 (Thresho (None, 39, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 37, 256)           196864    \n",
      "_________________________________________________________________\n",
      "thresholded_re_lu_4 (Thresho (None, 37, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 35, 256)           196864    \n",
      "_________________________________________________________________\n",
      "thresholded_re_lu_5 (Thresho (None, 35, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 33, 256)           196864    \n",
      "_________________________________________________________________\n",
      "thresholded_re_lu_6 (Thresho (None, 33, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 11, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2816)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              2884608   \n",
      "_________________________________________________________________\n",
      "thresholded_re_lu_7 (Thresho (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "thresholded_re_lu_8 (Thresho (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 8200      \n",
      "=================================================================\n",
      "Total params: 5,427,464\n",
      "Trainable params: 5,427,464\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build model ###\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Flatten\n",
    "from keras.layers import Convolution1D\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import ThresholdedReLU\n",
    "from keras.layers import Dropout\n",
    "import keras_metrics\n",
    "\n",
    "#vars\n",
    "input_size=config[\"data\"][\"input_size\"]\n",
    "alphabet_size=config[\"data\"][\"alphabet_size\"]\n",
    "embedding_size=config[\"char_cnn_zhang\"][\"embedding_size\"]\n",
    "conv_layers=config[\"char_cnn_zhang\"][\"conv_layers\"]\n",
    "fully_connected_layers=config[\"char_cnn_zhang\"][\"fully_connected_layers\"]\n",
    "num_of_classes=config[\"data\"][\"num_of_classes\"]\n",
    "threshold=config[\"char_cnn_zhang\"][\"threshold\"]\n",
    "dropout_p=config[\"char_cnn_zhang\"][\"dropout_p\"]\n",
    "print(\"Dropout param: \"+str(dropout_p))\n",
    "optimizer=config[\"char_cnn_zhang\"][\"optimizer\"]\n",
    "loss=config[\"char_cnn_zhang\"][\"loss\"]\n",
    "\n",
    "# Input layer\n",
    "inputs = Input(shape=(input_size,), name='sent_input', dtype='int64')\n",
    "        \n",
    "# Embedding layers\n",
    "x = Embedding(alphabet_size + 1, embedding_size, input_length=input_size)(inputs)\n",
    "\n",
    "# Convolution layers\n",
    "for cl in conv_layers:\n",
    "    x = Convolution1D(cl[0], cl[1])(x)\n",
    "    x = ThresholdedReLU(threshold)(x)\n",
    "    if cl[2] != -1:\n",
    "        x = MaxPooling1D(cl[2])(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "\n",
    "# Fully connected layers\n",
    "for fl in fully_connected_layers:\n",
    "    x = Dense(fl)(x)\n",
    "    x = ThresholdedReLU(threshold)(x)\n",
    "    x = Dropout(dropout_p)(x)\n",
    "        \n",
    "# Output layer\n",
    "predictions = Dense(num_of_classes, activation='softmax')(x)\n",
    "\n",
    "# Build and compile model\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[keras_metrics.precision(), keras_metrics.recall()]) #, metrics=['accuracy'])\n",
    "print(\"CharCNNZhang model built: \")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CharCNNZhang model: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 30069 samples, validate on 10023 samples\n",
      "Epoch 1/7\n",
      " - 246s - loss: 1.1519 - precision: 0.7236 - recall: 0.9079 - val_loss: 1.0142 - val_precision: 0.7753 - val_recall: 0.9889\n",
      "Epoch 2/7\n",
      " - 242s - loss: 0.8636 - precision: 0.8630 - recall: 0.9404 - val_loss: 0.7990 - val_precision: 0.9805 - val_recall: 0.9073\n",
      "Epoch 3/7\n",
      " - 241s - loss: 0.6828 - precision: 0.9692 - recall: 0.9788 - val_loss: 0.6638 - val_precision: 0.9659 - val_recall: 0.9826\n",
      "Epoch 4/7\n",
      " - 252s - loss: 0.6400 - precision: 0.9820 - recall: 0.9850 - val_loss: 0.6474 - val_precision: 0.9615 - val_recall: 0.9881\n",
      "Epoch 5/7\n",
      " - 244s - loss: 0.5777 - precision: 0.9861 - recall: 0.9900 - val_loss: 0.5883 - val_precision: 0.9720 - val_recall: 0.9815\n",
      "Epoch 6/7\n",
      " - 242s - loss: 0.5165 - precision: 0.9846 - recall: 0.9891 - val_loss: 0.5263 - val_precision: 0.9840 - val_recall: 0.9644\n",
      "Epoch 7/7\n",
      " - 238s - loss: 0.4424 - precision: 0.9870 - recall: 0.9871 - val_loss: 0.5218 - val_precision: 0.9754 - val_recall: 0.9806\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "\n",
    "epochs=config[\"training\"][\"epochs\"]\n",
    "batch_size=config[\"training\"][\"batch_size\"]\n",
    "checkpoint_every=config[\"training\"][\"checkpoint_every\"]\n",
    "    \n",
    "    \n",
    "print(\"Training CharCNNZhang model: \")\n",
    "model.fit(training_inputs, training_labels, validation_data=(validation_inputs, validation_labels),\n",
    "          epochs=epochs, batch_size=batch_size, verbose=2, callbacks=[])\n",
    "\n",
    "#model.fit(training_inputs, training_labels,\n",
    "#          epochs=epochs, batch_size=batch_size, verbose=2, callbacks=[])\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "tracking <tf.Variable 'Variable:0' shape=() dtype=int32, numpy=0> tp\n",
      "tracking <tf.Variable 'Variable:0' shape=() dtype=int32, numpy=0> fp\n",
      "tracking <tf.Variable 'Variable:0' shape=() dtype=int32, numpy=0> tp\n",
      "tracking <tf.Variable 'Variable:0' shape=() dtype=int32, numpy=0> fn\n",
      "10024/10024 [==============================] - 24s 2ms/step\n",
      "loss: 50.94%\n",
      "precision: 98.20%\n",
      "recall: 98.30%\n"
     ]
    }
   ],
   "source": [
    "# load json and create model\n",
    "from keras.models import model_from_json\n",
    "import keras_metrics\n",
    "\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(optimizer=optimizer, loss=loss, metrics=[keras_metrics.precision(), keras_metrics.recall()])\n",
    "score = loaded_model.evaluate(test_inputs, test_labels, batch_size=batch_size, verbose=1)\n",
    "\n",
    "for i in range(0,len(loaded_model.metrics_names)):\n",
    "    print(\"%s: %.2f%%\" % (loaded_model.metrics_names[i], score[i]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98      6971\n",
      "           1       0.37      0.29      0.33       374\n",
      "           2       0.30      0.63      0.41       439\n",
      "           3       0.52      0.47      0.50       442\n",
      "           4       0.60      0.67      0.63       476\n",
      "           5       0.55      0.49      0.52       462\n",
      "           6       0.23      0.06      0.09       470\n",
      "           7       0.38      0.26      0.31       390\n",
      "\n",
      "    accuracy                           0.81     10024\n",
      "   macro avg       0.49      0.48      0.47     10024\n",
      "weighted avg       0.81      0.81      0.81     10024\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "model = loaded_model\n",
    "y_test = test_labels\n",
    "Y_test = np.argmax(y_test, axis=1) # Convert one-hot to index\n",
    "y_pred = model.predict(test_inputs)\n",
    "y_pred = np.argmax(y_pred,axis=-1)\n",
    "print(classification_report(Y_test, y_pred))\n",
    "\n",
    "# for ECMLPKDD\n",
    "# valid = 0\n",
    "# xss =  1\n",
    "# sqlinjection = 2\n",
    "# ldapinjection  = 3\n",
    "# xpathinjection  = 4\n",
    "# pathtransversal = 5\n",
    "# oscommanding  = 6\n",
    "# ssi = 7\n",
    "\n",
    "# for  CISC\n",
    "# valid = 0\n",
    "# malicious = 1\n",
    "\n",
    "# for Morzeux_HttpParamsDataset\n",
    "# valid = 0\n",
    "# sqli  = 1\n",
    "# xss   = 2\n",
    "# path-traversal = 3\n",
    "# cmdi = 4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
